# -*- coding: utf-8 -*-
"""Titanic_survival

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1irB2cemVSJTwgDngyaSV8YRu3SylCFq5

1) Import libraries
"""

import os, re, json, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,
                             roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay)

warnings.filterwarnings("ignore")
plt.style.use("fivethirtyeight")

"""2) Load data"""

train = pd.read_csv("/content/drive/MyDrive/titanic_train.csv")
test = pd.read_csv("/content/drive/MyDrive/titanic_test.csv")

train_raw, test_raw = train.copy(), test.copy()
train.head()

"""3) Data Cleaning & Feature Engineering"""

def extract_title(name):
    match = re.search(r",\s*([^\.]+)\.", name)
    return match.group(1).strip() if match else "None"

def preprocess(df, is_train=True):
    df = df.copy()

    # Titles
    df["Title"] = df["Name"].apply(extract_title)
    title_map = {
        "Mlle":"Miss","Ms":"Miss","Mme":"Mrs",
        "Lady":"Rare","Countess":"Rare","Capt":"Rare","Col":"Rare","Don":"Rare",
        "Dr":"Rare","Major":"Rare","Rev":"Rare","Sir":"Rare","Jonkheer":"Rare","Dona":"Rare"
    }
    df["Title"] = df["Title"].replace(title_map)

    # Family features
    df["FamilySize"] = df["SibSp"] + df["Parch"] + 1
    df["IsAlone"] = (df["FamilySize"] == 1).astype(int)

    # Cabin Deck
    df["CabinDeck"] = df["Cabin"].astype(str).str[0]
    df.loc[df["CabinDeck"] == "n", "CabinDeck"] = "Unknown"

    # Ticket group size
    df["TicketGroupSize"] = df.groupby("Ticket")["Ticket"].transform("count")

    # Missing values
    df["Embarked"].fillna(df["Embarked"].mode()[0], inplace=True)
    df["Fare"].fillna(df.groupby("Pclass")["Fare"].transform("median"), inplace=True)

    # Age imputation
    age_grp = df.groupby(["Title", "Pclass", "Sex"])["Age"].median()
    def impute_age(row):
        if pd.isna(row["Age"]):
            return age_grp.loc[row["Title"], row["Pclass"], row["Sex"]]
        return row["Age"]
    df["Age"] = df.apply(impute_age, axis=1)

    # Drop unused columns
    drop_cols = ["Name","Cabin","Ticket","PassengerId"]
    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True)

    return df

train_clean = preprocess(train, is_train=True)
test_clean  = preprocess(test, is_train=False)

train_clean.head()

"""4) Save cleaned data"""

os.makedirs("reports/cleaned", exist_ok=True)
train_clean.to_csv("reports/cleaned/train_cleaned.csv", index=False)
test_clean.to_csv("reports/cleaned/test_cleaned.csv", index=False)
print("✅ Cleaned train and test datasets saved.")

"""5) Exploratory Data Analysis (EDA)"""

os.makedirs("reports/figures", exist_ok=True)

def bar_plot(series, title, fname):
    plt.figure(figsize=(6,4))
    series.value_counts().plot(kind="bar")
    plt.title(title)
    plt.ylabel("Count")
    plt.tight_layout()
    plt.savefig(os.path.join("reports/figures", fname))
    plt.show()

def survival_rate_by(feature):
    grp = train_clean.groupby(feature)["Survived"].mean().sort_values()
    plt.figure(figsize=(6,4))
    grp.plot(kind="bar")
    plt.title(f"Survival rate by {feature}")
    plt.ylabel("Survival rate")
    plt.tight_layout()
    plt.savefig(os.path.join("reports/figures", f"survival_by_{feature}.png"))
    plt.show()

# Example EDA
bar_plot(train_clean["Survived"], "Overall Survival", "survival.png")
for feat in ["Sex","Pclass","Embarked","IsAlone","Title","CabinDeck"]:
    survival_rate_by(feat)

"""6) Train / Validation Split"""

X = train_clean.drop(columns=["Survived"])
y = train_clean["Survived"]

X_train, X_valid, y_train, y_valid = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

X_train.shape, X_valid.shape

"""7) Model Building"""

numeric_features = ["Age","SibSp","Parch","Fare","FamilySize","TicketGroupSize"]
categorical_features = ["Pclass","Sex","Embarked","Title","IsAlone","CabinDeck"]

preprocessor = ColumnTransformer([
    ("num", StandardScaler(), numeric_features),
    ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features)
])

models = {
    "logreg": LogisticRegression(max_iter=300),
    "rf": RandomForestClassifier(random_state=42),
    "gb": GradientBoostingClassifier(random_state=42),
}

param_grid = {
    "logreg": {"model__C": [0.1, 1.0, 3.0]},
    "rf": {"model__n_estimators": [200, 500], "model__max_depth": [None, 5, 10]},
    "gb": {"model__n_estimators": [100, 300], "model__learning_rate": [0.05, 0.1], "model__max_depth": [2, 3]},
}

"""8) Training & Evaluation"""

from sklearn.model_selection import StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

results = []
best_model, best_name, best_f1 = None, None, -1

for name, mdl in models.items():
    pipe = Pipeline([("pre", preprocessor), ("model", mdl)])
    gs = GridSearchCV(pipe, param_grid[name], cv=cv, scoring="f1", n_jobs=-1)
    gs.fit(X_train, y_train)

    preds = gs.predict(X_valid)
    proba = gs.predict_proba(X_valid)[:,1] if hasattr(gs, "predict_proba") else None

    res = {
        "model": name,
        "best_params": gs.best_params_,
        "accuracy": accuracy_score(y_valid, preds),
        "precision": precision_score(y_valid, preds),
        "recall": recall_score(y_valid, preds),
        "f1": f1_score(y_valid, preds),
    }
    if proba is not None:
        res["roc_auc"] = roc_auc_score(y_valid, proba)

    results.append(res)

    if res["f1"] > best_f1:
        best_model, best_name, best_f1 = gs.best_estimator_, name, res["f1"]

pd.DataFrame(results)

"""9) Model Checking"""

print("\nBest Model:", best_name)
print(classification_report(y_valid, best_model.predict(X_valid)))

cm = confusion_matrix(y_valid, best_model.predict(X_valid))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["Died","Survived"], yticklabels=["Died","Survived"])
plt.title(f"Confusion Matrix - {best_name}")
plt.show()

"""10) Feature Importance (for tree models)"""

if hasattr(best_model.named_steps["model"], "feature_importances_"):
    importances = best_model.named_steps["model"].feature_importances_
    ohe = best_model.named_steps["pre"].named_transformers_["cat"]
    cat_names = list(ohe.get_feature_names_out(categorical_features))
    feat_names = numeric_features + cat_names
    pd.Series(importances, index=feat_names).nlargest(15).plot(kind="bar")
    plt.title("Top Feature Importances")
    plt.show()

"""11) Final Submission"""

final_preds = best_model.predict(test_clean)
submission = pd.DataFrame({
    "PassengerId": test_raw["PassengerId"],
    "Survived": final_preds
})
submission.to_csv("reports/final_submission.csv", index=False)
print("✅ Final submission file saved: reports/final_submission.csv")